# Research On Flexible Object Grasping Method Based on Visual Enhancement and Multi-Stage Collaboration
# åŸºäºè§†è§‰å¢å¼ºä¸å¤šé˜¶æ®µåä½œçš„æŸ”æ€§ç‰©ä½“æŠ“å–ç ”ç©¶

## ğŸ“Œ é¡¹ç›®ç®€ä»‹ | Project Introduction

**ğŸ‡¨ğŸ‡³ ä¸­æ–‡** æœ¬é¡¹ç›®é’ˆå¯¹é«˜åå…‰ã€ä½çº¹ç†çš„æŸ”æ€§ç‰©ä½“ï¼ˆå¦‚é€æ˜å¡‘æ–™è¢‹ï¼‰æŠ“å–éš¾é¢˜ï¼Œæå‡ºäº†ä¸€å¥—ç»“åˆ **è§†è§‰å¢å¼º (Visual Enhancement)** ä¸ **å¤šé˜¶æ®µåä½œ (Multi-Stage Collaboration)** çš„æœºå™¨äººæŠ“å–æ¡†æ¶ã€‚é€šè¿‡â€œå±•å¼€-æŠ“å–â€çš„ç²¾å‡†é˜¶æ®µè½¬æ¢ï¼Œæ˜¾è‘—æå‡äº†æœåŠ¡æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„æ“ä½œç¨³å®šæ€§ã€‚

**ğŸ‡¬ğŸ‡§ English** This project addresses the challenge of grasping reflective, low-texture flexible objects (e.g., plastic bags) by proposing a framework that combines **Visual Enhancement** and **Multi-Stage Collaboration**. By enabling accurate "unfolding-grasping" transitions, it significantly improves the stability of service robots in complex scenarios.

---

## ğŸš€ æ ¸å¿ƒäº®ç‚¹ | Key Contributions

| æ¨¡å— / Module | æè¿° / Description |
| :--- | :--- |
| **å¤šé˜¶æ®µåä½œæ¡†æ¶**<br>Multi-Stage Collaboration | **CN:** å°†ä»»åŠ¡åˆ†è§£ä¸ºåŸºäºæ¨¡ä»¿å­¦ä¹ çš„â€œå±•å¼€é˜¶æ®µâ€å’ŒåŸºäºè§†è§‰ä¼ºæœçš„â€œæŠ“å–é˜¶æ®µâ€ã€‚<br>**EN:** Decomposes the task into an imitation learning-based "Unfolding Stage" and a visual servoing-based "Grasping Stage". |
| **è§†è§‰å¢å¼ºæµæ°´çº¿**<br>Visual Enhancement Pipeline | **CN:** é€šè¿‡é«˜æ–¯å¹³æ»‘ã€å¯¹æ¯”åº¦è°ƒæ•´å’Œå½¢æ€å­¦å¤„ç†ï¼Œå°†è¾¹ç¼˜æ£€æµ‹å‡†ç¡®ç‡æå‡è‡³ **90%** ä»¥ä¸Šã€‚<br>**EN:** Improves edge detection accuracy to over **90%** via Gaussian smoothing, contrast adjustment, and morphological processing. |
| **ç²¾å‡†è½¬æ¢æœºåˆ¶**<br>Phase Transition | **CN:** é‡‡ç”¨ä¸‰çº§è§¦å‘ç³»ç»Ÿï¼Œå®æ—¶ç›‘æ§æœºæ¢°è‡‚å…³èŠ‚æ•°æ®ï¼Œç¡®ä¿å„é˜¶æ®µæ— ç¼è¡”æ¥ã€‚<br>**EN:** Adopts a three-level trigger system monitoring joint data in real-time to ensure seamless transitions. |
| **æ€§èƒ½æå‡**<br>Performance Boost | **CN:** å®éªŒè¯æ˜ï¼Œæœ¬æ¡†æ¶çš„æŠ“å–æˆåŠŸç‡è¾¾åˆ° **83%**ï¼Œè¾ƒä¼ ç»Ÿæ–¹æ³•æå‡äº† **35%**ã€‚<br>**EN:** Experiments show a grasping success rate of **83%**, a **35%** improvement over traditional methods. |

---

## ğŸ¥ æ¼”ç¤ºè§†é¢‘ | Demo Video

> **Note:** ä»¥ä¸‹è§†é¢‘å±•ç¤ºäº†ä»å±•å¼€åˆ°æŠ“å–çš„å®Œæ•´æµç¨‹åŠç®—æ³•çš„é²æ£’æ€§æµ‹è¯•ã€‚
> The following videos demonstrate the full process from unfolding to grasping and robustness tests.

### 1. é˜¶æ®µ 1 ä¸é˜¶æ®µ 2 è½¬æ¢ (Phase Transition)
**CN:** å±•ç¤ºäº†ç³»ç»Ÿé€šè¿‡å®æ—¶ç›‘æµ‹æœºæ¢°è‡‚å…³èŠ‚è§’åº¦ï¼ˆé«˜ç²¾åº¦ï¼‰ï¼Œåœ¨å®Œæˆè¢‹å­å±•å¼€åè‡ªåŠ¨è§¦å‘ä»â€œæ¨¡ä»¿å­¦ä¹ â€åˆ°â€œè§†è§‰ä¼ºæœæŠ“å–â€çš„å¹³æ»‘è½¬æ¢ã€‚  
**EN:** Demonstrates the smooth transition from Stage 1 (imitation learning) to Stage 2 (visual servoing) triggered by real-time joint angle monitoring with high precision.

https://github.com/user-attachments/assets/4603b509-eaeb-4aa5-a8ca-4bf48f7e3d74

https://github.com/user-attachments/assets/dd9c6639-5c8a-4a28-a563-27a95cc84f63

https://github.com/user-attachments/assets/b4c0b1f5-3735-443a-9ce6-d556ef0892df

### 2. å…¨æµç¨‹æ¼”ç¤º (Full Task: From Unfolding to Placement)
**CN:** æ¼”ç¤ºäº†ä»åŒè‡‚åä½œå±•å¼€åå…‰å¡‘æ–™è¢‹åˆ°æœ€ç»ˆç²¾å‡†æ”¾å…¥ç‰©å“ï¼ˆå¦‚æ³°è¿ªç†Šï¼‰çš„å®Œæ•´é—­ç¯æ§åˆ¶è¿‡ç¨‹ã€‚  
**EN:** Illustrates the complete closed-loop control from dual-arm unfolding of reflective bags to the precise placement of objects (e.g., a teddy bear).

https://github.com/user-attachments/assets/84d46608-2374-4f60-b5bf-87ac89c7346e

### 3. å¤šæ ·åŒ–é¢œè‰²é€‚é… (Color Generalization)
**CN:** éªŒè¯äº†è§†è§‰å¢å¼ºç®—æ³•å¯¹ä¸åŒé¢œè‰²ã€é«˜åå…‰æŸ”æ€§ç‰©ä½“çš„ç¨³å¥æ€§ï¼Œé€šè¿‡å‚æ•°ä¼˜åŒ–æ˜¾è‘—æŠ‘åˆ¶äº†å…‰å½±å¹²æ‰°ã€‚  
**EN:** Validates the robustness of visual enhancement algorithms across different colors and highly reflective flexible objects by suppressing lighting interference.

https://github.com/user-attachments/assets/7d8bd20f-4a2c-40ec-834e-5635cf33dba4

https://github.com/user-attachments/assets/3c84b179-5823-441b-bf56-33200c4242fa

### 4. é€šç”¨æ€§æµ‹è¯•ï¼šåˆšæ€§ç‰©ä½“æŠ“å– (Generalization: Rigid Object)
**CN:** è¯æ˜äº†è¯¥æ¡†æ¶åœ¨å¤„ç†éç»“æ„åŒ–æŸ”æ€§ç‰©ä½“çš„åŒæ—¶ï¼ŒåŒæ ·å…·å¤‡å¯¹æ™®é€šåˆšæ€§ç‰©ä½“çš„ç¨³å®šæŠ“å–èƒ½åŠ›ã€‚  
**EN:** Demonstrates that the framework maintains stable grasping capabilities for rigid objects while excelling at unstructured flexible object manipulation.

https://github.com/user-attachments/assets/134cfb46-261f-4229-b06f-ba3fa7a28b49

### 5. æ•°æ®é‡‡é›†è¿‡ç¨‹ (Data Collection & Training)
**CN:** å±•ç¤ºäº†ç”¨äºè®­ç»ƒ Transformer è¡Œä¸ºå…‹éš†æ¨¡å‹çš„ä¸“å®¶æ¼”ç¤ºæ•°æ®é‡‡é›†è¿‡ç¨‹ï¼Œä¸ºæ¨¡ä»¿å­¦ä¹ æä¾›é«˜è´¨é‡è¾“å…¥ã€‚  
**EN:** Shows the expert demonstration data collection process used to train the Transformer-based behavioral cloning model.

https://github.com/user-attachments/assets/c1db5af7-147a-4636-ae44-fd71ac63577b

---

## ğŸ› ï¸ ç³»ç»Ÿæ„æˆ | System Setup

| ç»„ä»¶ / Component | è§„æ ¼ / Specification |
| :--- | :--- |
| **æœºæ¢°è‡‚** (Robots) | 2 Ã— DOBOT Nova5 (6-DOF) |
| **ä¼ æ„Ÿå™¨** (Sensors) | 4 Ã— Intel RealSense D405 Depth Cameras |
| **æœ«ç«¯æ‰§è¡Œå™¨** (Gripper) | Custom 3D-printed dual-finger soft gripper (3D æ‰“å°åŒæŒ‡è½¯èƒ¶æŠ“å–æ‰‹) |
| **æ§åˆ¶ç®—æ³•** (Algorithms)| Transformer-based Behavioral Cloning & YOLO v8 |

---

## ğŸ“– å¼•ç”¨ | Citation

If you find this work helpful, please consider citing:

```bibtex
@article{aga2025research,
  title={Research On Flexible Object Grasping Method Based on Visual Enhancement and Multi-Stage Collaboration},
  author={Aga, Cila and Cao, Zhijun and Chi, Junchen and Liu, Jin and Wang, Chaoqun and Fu, Tianyu and Song, Rui},
  journal={Procedia Computer Science},
  volume={271},
  pages={7--13},
  year={2025}
}
